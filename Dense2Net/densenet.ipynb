{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dense2net.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"wfC4uOSnm6Pl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":163},"outputId":"19af032f-abf1-40c3-a0e2-37e8a0e97047","executionInfo":{"status":"ok","timestamp":1556046830461,"user_tz":240,"elapsed":24885,"user":{"displayName":"Tanner Songkakul","photoUrl":"","userId":"16302517323990624028"}}},"cell_type":"code","source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive/')\n","# Navigate to the directory containing this notebook\n","%cd \"/content/drive/My Drive/StarliperSongkakul-Project3/pytorch-cifar-master\"\n","\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n","/content/drive/My Drive/StarliperSongkakul-Project3/pytorch-cifar-master\n","dense2net.ipynb  LICENSE  main.py  models  README.md  utils.py\n"],"name":"stdout"}]},{"metadata":{"id":"43Uzbc66oPnS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"9c054c62-7a79-48fd-a5bd-979aaf95718b","executionInfo":{"status":"ok","timestamp":1556047701049,"user_tz":240,"elapsed":572,"user":{"displayName":"Tanner Songkakul","photoUrl":"","userId":"16302517323990624028"}}},"cell_type":"code","source":["'''DenseNet in PyTorch.'''\n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Bottleneck(nn.Module):\n","    def __init__(self, in_planes, growth_rate):\n","        super(Bottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n","        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n","\n","    def forward(self, x):\n","        out = self.conv1(F.relu(self.bn1(x)))\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = torch.cat([out,x], 1)\n","        return out\n","\n","\n","class Transition(nn.Module):\n","    def __init__(self, in_planes, out_planes):\n","        super(Transition, self).__init__()\n","        self.bn = nn.BatchNorm2d(in_planes)\n","        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n","\n","    def forward(self, x):\n","        out = self.conv(F.relu(self.bn(x)))\n","        out = F.avg_pool2d(out, 2)\n","        return out\n","\n","\n","class DenseNet(nn.Module):\n","    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n","        super(DenseNet, self).__init__()\n","        self.growth_rate = growth_rate\n","\n","        num_planes = 2*growth_rate\n","        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n","\n","        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n","        num_planes += nblocks[0]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans1 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n","        num_planes += nblocks[1]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans2 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n","        num_planes += nblocks[2]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans3 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n","        num_planes += nblocks[3]*growth_rate\n","\n","        self.bn = nn.BatchNorm2d(num_planes)\n","        self.linear = nn.Linear(num_planes, num_classes)\n","\n","    def _make_dense_layers(self, block, in_planes, nblock):\n","        layers = []\n","        for i in range(nblock):\n","            layers.append(block(in_planes, self.growth_rate))\n","            in_planes += self.growth_rate\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.trans1(self.dense1(out))\n","        out = self.trans2(self.dense2(out))\n","        out = self.trans3(self.dense3(out))\n","        out = self.dense4(out)\n","        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","def DenseNet121():\n","    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n","\n","def DenseNet169():\n","    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n","\n","def DenseNet201():\n","    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n","\n","def DenseNet161():\n","    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n","\n","def densenet_cifar():\n","    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n","\n","def test():\n","    net = densenet_cifar()\n","    x = torch.randn(1,3,32,32)\n","    y = net(x)\n","    print(y)\n","\n","    \n","print('Defined DenseNet')\n","# test()\n","\n","def progress_bar(current, total, msg=None):\n","    global last_time, begin_time\n","    if current == 0:\n","        begin_time = time.time()  # Reset for new bar.\n","\n","    cur_len = int(65*current/total)\n","    rest_len = int(65 - cur_len) - 1\n","\n","    sys.stdout.write(' [')\n","    for i in range(cur_len):\n","        sys.stdout.write('=')\n","    sys.stdout.write('>')\n","    for i in range(rest_len):\n","        sys.stdout.write('.')\n","    sys.stdout.write(']')\n","\n","    cur_time = time.time()\n","    step_time = cur_time - last_time\n","    last_time = cur_time\n","    tot_time = cur_time - begin_time\n","\n","    L = []\n","    L.append('  Step: %s' % format_time(step_time))\n","    L.append(' | Tot: %s' % format_time(tot_time))\n","    if msg:\n","        L.append(' | ' + msg)\n","\n","    msg = ''.join(L)\n","    sys.stdout.write(msg)\n","    for i in range(term_width-int(65)-len(msg)-3):\n","        sys.stdout.write(' ')\n","\n","    # Go back to the center of the bar.\n","    for i in range(term_width-int(65/2)+2):\n","        sys.stdout.write('\\b')\n","    sys.stdout.write(' %d/%d ' % (current+1, total))\n","\n","    if current < total-1:\n","        sys.stdout.write('\\r')\n","    else:\n","        sys.stdout.write('\\n')\n","    sys.stdout.flush()\n","    \n","def format_time(seconds):\n","    days = int(seconds / 3600/24)\n","    seconds = seconds - days*3600*24\n","    hours = int(seconds / 3600)\n","    seconds = seconds - hours*3600\n","    minutes = int(seconds / 60)\n","    seconds = seconds - minutes*60\n","    secondsf = int(seconds)\n","    seconds = seconds - secondsf\n","    millis = int(seconds*1000)\n","\n","    f = ''\n","    i = 1\n","    if days > 0:\n","        f += str(days) + 'D'\n","        i += 1\n","    if hours > 0 and i <= 2:\n","        f += str(hours) + 'h'\n","        i += 1\n","    if minutes > 0 and i <= 2:\n","        f += str(minutes) + 'm'\n","        i += 1\n","    if secondsf > 0 and i <= 2:\n","        f += str(secondsf) + 's'\n","        i += 1\n","    if millis > 0 and i <= 2:\n","        f += str(millis) + 'ms'\n","        i += 1\n","    if f == '':\n","        f = '0ms'\n","    return f"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Defined DenseNet\n"],"name":"stdout"}]},{"metadata":{"id":"U_1SxNejmlxn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3887},"outputId":"89934c3b-52d8-4d78-fbee-46dfbcc898b3"},"cell_type":"code","source":["'''Train CIFAR10 with PyTorch.'''\n","from __future__ import print_function\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import time\n","import sys\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","best_acc = 0  # best test accuracy\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","# Data\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","# Model\n","print('==> Building model..')\n","# net = VGG('VGG19')\n","# net = ResNet18()\n","# net = PreActResNet18()\n","# net = GoogLeNet()\n","net = DenseNet121()\n","# net = ResNeXt29_2x64d()\n","# net = MobileNet()\n","# net = MobileNetV2()\n","# net = DPN92()\n","# net = ShuffleNetG2()\n","# net = SENet18()\n","#net = ShuffleNetV2(1)\n","net = net.to(device)\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","''\n","\n","learning_rate = 0.1\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","        if(batch_idx%25==0) or batch_idx==len(trainloader)-1: \n","          print(batch_idx+1, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","        #progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","        #    % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","            if(batch_idx%25==0) or batch_idx==len(testloader)-1: \n","                print(batch_idx+1, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","            #progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir('checkpoint'):\n","            os.mkdir('checkpoint')\n","        torch.save(state, './checkpoint/ckpt.t7')\n","        best_acc = acc\n","\n","TOTAL_BAR_LENGTH = 65.\n","last_time = time.time()\n","begin_time = last_time\n","term_width= 1000\n","\n","for epoch in range(start_epoch, start_epoch+200):\n","    train(epoch)\n","    test(epoch)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n","==> Building model..\n","\n","Epoch: 0\n","1 391 Loss: 2.307 | Acc: 10.938% (14/128)\n","26 391 Loss: 2.083 | Acc: 22.296% (742/3328)\n","51 391 Loss: 1.997 | Acc: 25.980% (1696/6528)\n","76 391 Loss: 1.936 | Acc: 27.827% (2707/9728)\n","101 391 Loss: 1.884 | Acc: 29.680% (3837/12928)\n","126 391 Loss: 1.842 | Acc: 31.430% (5069/16128)\n","151 391 Loss: 1.801 | Acc: 32.859% (6351/19328)\n","176 391 Loss: 1.765 | Acc: 34.264% (7719/22528)\n","201 391 Loss: 1.738 | Acc: 35.265% (9073/25728)\n","226 391 Loss: 1.711 | Acc: 36.387% (10526/28928)\n","251 391 Loss: 1.682 | Acc: 37.456% (12034/32128)\n","276 391 Loss: 1.650 | Acc: 38.635% (13649/35328)\n","301 391 Loss: 1.621 | Acc: 39.885% (15367/38528)\n","326 391 Loss: 1.596 | Acc: 40.781% (17017/41728)\n","351 391 Loss: 1.572 | Acc: 41.738% (18752/44928)\n","376 391 Loss: 1.551 | Acc: 42.555% (20481/48128)\n","391 391 Loss: 1.537 | Acc: 43.034% (21517/50000)\n","1 100 Loss: 1.377 | Acc: 53.000% (53/100)\n","26 100 Loss: 1.444 | Acc: 50.885% (1323/2600)\n","51 100 Loss: 1.429 | Acc: 51.020% (2602/5100)\n","76 100 Loss: 1.440 | Acc: 50.342% (3826/7600)\n","100 100 Loss: 1.445 | Acc: 50.150% (5015/10000)\n","Saving..\n","\n","Epoch: 1\n","1 391 Loss: 1.082 | Acc: 63.281% (81/128)\n","26 391 Loss: 1.168 | Acc: 57.181% (1903/3328)\n","51 391 Loss: 1.112 | Acc: 59.942% (3913/6528)\n","76 391 Loss: 1.115 | Acc: 60.053% (5842/9728)\n","101 391 Loss: 1.103 | Acc: 60.342% (7801/12928)\n","126 391 Loss: 1.098 | Acc: 60.807% (9807/16128)\n","151 391 Loss: 1.085 | Acc: 61.124% (11814/19328)\n","176 391 Loss: 1.073 | Acc: 61.612% (13880/22528)\n","201 391 Loss: 1.062 | Acc: 62.002% (15952/25728)\n","226 391 Loss: 1.053 | Acc: 62.389% (18048/28928)\n","251 391 Loss: 1.044 | Acc: 62.696% (20143/32128)\n","276 391 Loss: 1.035 | Acc: 63.080% (22285/35328)\n","301 391 Loss: 1.026 | Acc: 63.367% (24414/38528)\n","326 391 Loss: 1.018 | Acc: 63.648% (26559/41728)\n","351 391 Loss: 1.010 | Acc: 63.956% (28734/44928)\n","376 391 Loss: 0.998 | Acc: 64.380% (30985/48128)\n","391 391 Loss: 0.995 | Acc: 64.496% (32248/50000)\n","1 100 Loss: 1.083 | Acc: 58.000% (58/100)\n","26 100 Loss: 1.002 | Acc: 64.308% (1672/2600)\n","51 100 Loss: 0.989 | Acc: 65.275% (3329/5100)\n","76 100 Loss: 0.994 | Acc: 65.092% (4947/7600)\n","100 100 Loss: 0.997 | Acc: 65.050% (6505/10000)\n","Saving..\n","\n","Epoch: 2\n","1 391 Loss: 0.789 | Acc: 69.531% (89/128)\n","26 391 Loss: 0.837 | Acc: 71.274% (2372/3328)\n","51 391 Loss: 0.825 | Acc: 71.232% (4650/6528)\n","76 391 Loss: 0.825 | Acc: 71.525% (6958/9728)\n","101 391 Loss: 0.824 | Acc: 71.643% (9262/12928)\n","126 391 Loss: 0.822 | Acc: 71.460% (11525/16128)\n","151 391 Loss: 0.821 | Acc: 71.316% (13784/19328)\n","176 391 Loss: 0.811 | Acc: 71.564% (16122/22528)\n","201 391 Loss: 0.807 | Acc: 71.669% (18439/25728)\n","226 391 Loss: 0.797 | Acc: 72.006% (20830/28928)\n","251 391 Loss: 0.793 | Acc: 72.180% (23190/32128)\n","276 391 Loss: 0.792 | Acc: 72.263% (25529/35328)\n","301 391 Loss: 0.785 | Acc: 72.537% (27947/38528)\n","326 391 Loss: 0.781 | Acc: 72.627% (30306/41728)\n","351 391 Loss: 0.775 | Acc: 72.825% (32719/44928)\n","376 391 Loss: 0.771 | Acc: 73.014% (35140/48128)\n","391 391 Loss: 0.769 | Acc: 73.068% (36534/50000)\n","1 100 Loss: 0.761 | Acc: 71.000% (71/100)\n","26 100 Loss: 0.806 | Acc: 72.308% (1880/2600)\n","51 100 Loss: 0.783 | Acc: 73.157% (3731/5100)\n","76 100 Loss: 0.779 | Acc: 72.737% (5528/7600)\n","100 100 Loss: 0.789 | Acc: 72.510% (7251/10000)\n","Saving..\n","\n","Epoch: 3\n","1 391 Loss: 0.530 | Acc: 82.812% (106/128)\n","26 391 Loss: 0.671 | Acc: 76.593% (2549/3328)\n","51 391 Loss: 0.665 | Acc: 76.639% (5003/6528)\n","76 391 Loss: 0.658 | Acc: 76.881% (7479/9728)\n","101 391 Loss: 0.661 | Acc: 76.833% (9933/12928)\n","126 391 Loss: 0.657 | Acc: 76.972% (12414/16128)\n","151 391 Loss: 0.656 | Acc: 77.070% (14896/19328)\n","176 391 Loss: 0.655 | Acc: 77.122% (17374/22528)\n","201 391 Loss: 0.650 | Acc: 77.317% (19892/25728)\n","226 391 Loss: 0.646 | Acc: 77.430% (22399/28928)\n","251 391 Loss: 0.643 | Acc: 77.493% (24897/32128)\n","276 391 Loss: 0.642 | Acc: 77.613% (27419/35328)\n","301 391 Loss: 0.637 | Acc: 77.743% (29953/38528)\n","326 391 Loss: 0.633 | Acc: 77.955% (32529/41728)\n","351 391 Loss: 0.631 | Acc: 77.974% (35032/44928)\n","376 391 Loss: 0.627 | Acc: 78.104% (37590/48128)\n","391 391 Loss: 0.626 | Acc: 78.176% (39088/50000)\n","1 100 Loss: 0.901 | Acc: 71.000% (71/100)\n","26 100 Loss: 0.831 | Acc: 71.846% (1868/2600)\n","51 100 Loss: 0.816 | Acc: 72.529% (3699/5100)\n","76 100 Loss: 0.808 | Acc: 72.934% (5543/7600)\n","100 100 Loss: 0.812 | Acc: 72.750% (7275/10000)\n","Saving..\n","\n","Epoch: 4\n","1 391 Loss: 0.660 | Acc: 75.781% (97/128)\n","26 391 Loss: 0.563 | Acc: 80.319% (2673/3328)\n","51 391 Loss: 0.551 | Acc: 80.821% (5276/6528)\n","76 391 Loss: 0.557 | Acc: 80.602% (7841/9728)\n","101 391 Loss: 0.570 | Acc: 80.121% (10358/12928)\n","126 391 Loss: 0.567 | Acc: 80.196% (12934/16128)\n","151 391 Loss: 0.567 | Acc: 80.345% (15529/19328)\n","176 391 Loss: 0.562 | Acc: 80.615% (18161/22528)\n","201 391 Loss: 0.558 | Acc: 80.702% (20763/25728)\n","226 391 Loss: 0.559 | Acc: 80.645% (23329/28928)\n","251 391 Loss: 0.558 | Acc: 80.693% (25925/32128)\n","276 391 Loss: 0.560 | Acc: 80.675% (28501/35328)\n","301 391 Loss: 0.560 | Acc: 80.695% (31090/38528)\n","326 391 Loss: 0.560 | Acc: 80.644% (33651/41728)\n","351 391 Loss: 0.559 | Acc: 80.647% (36233/44928)\n","376 391 Loss: 0.556 | Acc: 80.735% (38856/48128)\n","391 391 Loss: 0.555 | Acc: 80.782% (40391/50000)\n","1 100 Loss: 0.591 | Acc: 79.000% (79/100)\n","26 100 Loss: 0.612 | Acc: 79.000% (2054/2600)\n","51 100 Loss: 0.591 | Acc: 80.098% (4085/5100)\n","76 100 Loss: 0.593 | Acc: 79.882% (6071/7600)\n","100 100 Loss: 0.592 | Acc: 80.010% (8001/10000)\n","Saving..\n","\n","Epoch: 5\n","1 391 Loss: 0.549 | Acc: 80.469% (103/128)\n","26 391 Loss: 0.499 | Acc: 82.993% (2762/3328)\n","51 391 Loss: 0.512 | Acc: 82.782% (5404/6528)\n","76 391 Loss: 0.513 | Acc: 82.484% (8024/9728)\n","101 391 Loss: 0.508 | Acc: 82.642% (10684/12928)\n","126 391 Loss: 0.506 | Acc: 82.608% (13323/16128)\n","151 391 Loss: 0.510 | Acc: 82.414% (15929/19328)\n","176 391 Loss: 0.512 | Acc: 82.271% (18534/22528)\n","201 391 Loss: 0.512 | Acc: 82.311% (21177/25728)\n","226 391 Loss: 0.508 | Acc: 82.460% (23854/28928)\n","251 391 Loss: 0.509 | Acc: 82.371% (26464/32128)\n","276 391 Loss: 0.511 | Acc: 82.334% (29087/35328)\n","301 391 Loss: 0.511 | Acc: 82.350% (31728/38528)\n","326 391 Loss: 0.511 | Acc: 82.415% (34390/41728)\n","351 391 Loss: 0.509 | Acc: 82.505% (37068/44928)\n","376 391 Loss: 0.509 | Acc: 82.486% (39699/48128)\n","391 391 Loss: 0.508 | Acc: 82.522% (41261/50000)\n","1 100 Loss: 0.568 | Acc: 79.000% (79/100)\n","26 100 Loss: 0.590 | Acc: 80.654% (2097/2600)\n","51 100 Loss: 0.584 | Acc: 81.157% (4139/5100)\n","76 100 Loss: 0.583 | Acc: 80.947% (6152/7600)\n","100 100 Loss: 0.590 | Acc: 80.570% (8057/10000)\n","Saving..\n","\n","Epoch: 6\n","1 391 Loss: 0.526 | Acc: 81.250% (104/128)\n","26 391 Loss: 0.505 | Acc: 82.843% (2757/3328)\n","51 391 Loss: 0.491 | Acc: 83.195% (5431/6528)\n","76 391 Loss: 0.494 | Acc: 82.823% (8057/9728)\n","101 391 Loss: 0.494 | Acc: 82.882% (10715/12928)\n","126 391 Loss: 0.492 | Acc: 83.061% (13396/16128)\n","151 391 Loss: 0.492 | Acc: 83.087% (16059/19328)\n","176 391 Loss: 0.494 | Acc: 83.003% (18699/22528)\n","201 391 Loss: 0.495 | Acc: 82.925% (21335/25728)\n","226 391 Loss: 0.490 | Acc: 83.086% (24035/28928)\n","251 391 Loss: 0.489 | Acc: 83.062% (26686/32128)\n","276 391 Loss: 0.489 | Acc: 83.101% (29358/35328)\n","301 391 Loss: 0.487 | Acc: 83.225% (32065/38528)\n","326 391 Loss: 0.488 | Acc: 83.208% (34721/41728)\n","351 391 Loss: 0.487 | Acc: 83.244% (37400/44928)\n","376 391 Loss: 0.485 | Acc: 83.322% (40101/48128)\n","391 391 Loss: 0.484 | Acc: 83.352% (41676/50000)\n","1 100 Loss: 0.440 | Acc: 81.000% (81/100)\n","26 100 Loss: 0.586 | Acc: 79.115% (2057/2600)\n","51 100 Loss: 0.588 | Acc: 79.490% (4054/5100)\n","76 100 Loss: 0.587 | Acc: 79.987% (6079/7600)\n","100 100 Loss: 0.585 | Acc: 80.140% (8014/10000)\n","\n","Epoch: 7\n","1 391 Loss: 0.499 | Acc: 82.031% (105/128)\n","26 391 Loss: 0.436 | Acc: 84.796% (2822/3328)\n","51 391 Loss: 0.448 | Acc: 84.314% (5504/6528)\n","76 391 Loss: 0.448 | Acc: 84.272% (8198/9728)\n","101 391 Loss: 0.455 | Acc: 84.050% (10866/12928)\n","126 391 Loss: 0.458 | Acc: 84.040% (13554/16128)\n","151 391 Loss: 0.461 | Acc: 83.951% (16226/19328)\n","176 391 Loss: 0.460 | Acc: 84.038% (18932/22528)\n","201 391 Loss: 0.461 | Acc: 84.146% (21649/25728)\n","226 391 Loss: 0.456 | Acc: 84.320% (24392/28928)\n","251 391 Loss: 0.454 | Acc: 84.372% (27107/32128)\n","276 391 Loss: 0.455 | Acc: 84.333% (29793/35328)\n","301 391 Loss: 0.458 | Acc: 84.253% (32461/38528)\n","326 391 Loss: 0.459 | Acc: 84.241% (35152/41728)\n","351 391 Loss: 0.460 | Acc: 84.190% (37825/44928)\n","376 391 Loss: 0.461 | Acc: 84.138% (40494/48128)\n","391 391 Loss: 0.461 | Acc: 84.146% (42073/50000)\n","1 100 Loss: 0.853 | Acc: 72.000% (72/100)\n","26 100 Loss: 0.816 | Acc: 72.115% (1875/2600)\n","51 100 Loss: 0.802 | Acc: 72.510% (3698/5100)\n","76 100 Loss: 0.809 | Acc: 72.382% (5501/7600)\n","100 100 Loss: 0.800 | Acc: 72.820% (7282/10000)\n","\n","Epoch: 8\n","1 391 Loss: 0.448 | Acc: 82.031% (105/128)\n","26 391 Loss: 0.439 | Acc: 85.036% (2830/3328)\n","51 391 Loss: 0.443 | Acc: 85.018% (5550/6528)\n","76 391 Loss: 0.436 | Acc: 85.269% (8295/9728)\n","101 391 Loss: 0.433 | Acc: 85.319% (11030/12928)\n","126 391 Loss: 0.439 | Acc: 85.045% (13716/16128)\n","151 391 Loss: 0.438 | Acc: 84.918% (16413/19328)\n","176 391 Loss: 0.438 | Acc: 84.921% (19131/22528)\n","201 391 Loss: 0.435 | Acc: 85.028% (21876/25728)\n","226 391 Loss: 0.437 | Acc: 84.952% (24575/28928)\n","251 391 Loss: 0.441 | Acc: 84.854% (27262/32128)\n"],"name":"stdout"}]},{"metadata":{"id":"3dgMtZW_psTR","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}