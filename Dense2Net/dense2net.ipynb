{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dense2net.ipynb","version":"0.3.2","provenance":[{"file_id":"1RPow1iaBOxFxDH_RL_jDzY952yQoXl5i","timestamp":1556218753021}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ehv8L1aCR4gg","colab_type":"text"},"source":["# Dense2Net\n","Implementation incorporating the Res2Net architecture into dense net.\n","\n","DenseNet Paper: https://arxiv.org/abs/1608.06993\n","\n","Res2Net Paper: https://arxiv.org/abs/1904.01169\n","\n","DenseNet Cifar10 code from\n","https://github.com/kuangliu/pytorch-cifar\n","and https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n","\n","Res2Net code from https://github.com/lxtGH/OctaveConv_pytorch/blob/master/nn/res2net.py"]},{"cell_type":"markdown","metadata":{"id":"YWISDATvVCCn","colab_type":"text"},"source":["The below cell is only necessary when running in a Colaboratory notebook"]},{"cell_type":"code","metadata":{"id":"wfC4uOSnm6Pl","colab_type":"code","outputId":"a3522abc-0bd4-45d4-b782-ca474076e932","executionInfo":{"status":"ok","timestamp":1556478169181,"user_tz":240,"elapsed":28207,"user":{"displayName":"Tanner Songkakul","photoUrl":"","userId":"16302517323990624028"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive/')\n","# Navigate to the directory containing this notebook\n","%cd \"/content/drive/My Drive/StarliperSongkakul-Project3/Dense2Net\"\n","\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n","/content/drive/My Drive/StarliperSongkakul-Project3/Dense2Net\n","checkpoint  dense2net.ipynb  LICENSE  models\t utils.py\n","data\t    densenet.ipynb   main.py  README.md\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fs9futMsVJtu","colab_type":"text"},"source":["## Implementation of the Densenet and Res2net blocks\n","**To Do**\n","\n","\n","*   Comment and better understand structure\n","*   Draw exact underlying structure for Cifar10\n","\n"]},{"cell_type":"code","metadata":{"id":"43Uzbc66oPnS","colab_type":"code","outputId":"a89fab3c-7306-43b0-c243-8587b5c17bb5","executionInfo":{"status":"ok","timestamp":1556842003421,"user_tz":240,"elapsed":1931,"user":{"displayName":"Tanner Songkakul","photoUrl":"","userId":"16302517323990624028"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["'''DenseNet in PyTorch.'''\n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1):  \n","    #returns a 3x3 2d convolution, used in Res2Net sub-convolutions\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, groups=groups, bias=False)\n","\n","class Res2Net_block(nn.Module):\n","    #Res2net bottleneck block\n","    def __init__(self, planes, scale=1, stride=1, groups=1, norm_layer=None):\n","        super(Res2Net_block, self).__init__()\n","        \n","        self.relu = nn.ReLU(inplace=True)\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","            \n","        self.scale = scale\n","        ch_per_sub = planes // self.scale\n","        ch_res = planes % self.scale    \n","        self.chunks  = [ch_per_sub * i + ch_res for i in range(1, scale + 1)]\n","        self.conv_blocks = self.get_sub_convs(ch_per_sub, norm_layer, stride, groups)\n","        \n","    def forward(self, x):\n","        sub_convs = []\n","        sub_convs.append(x[:, :self.chunks[0]])\n","        sub_convs.append(self.conv_blocks[0](x[:, self.chunks[0]: self.chunks[1]]))\n","        for s in range(2, self.scale):\n","            sub_x = x[:, self.chunks[s-1]: self.chunks[s]]\n","            sub_x += sub_convs[-1]\n","            sub_convs.append(self.conv_blocks[s-1](sub_x))\n","\n","        return torch.cat(sub_convs, dim=1)\n","    \n","    def get_sub_convs(self, ch_per_sub, norm_layer, stride, groups):\n","        layers = []\n","        for _ in range(1, self.scale):\n","            layers.append(nn.Sequential(\n","                conv3x3(ch_per_sub, ch_per_sub, stride, groups),\n","                norm_layer(ch_per_sub), self.relu))\n","        \n","        return nn.Sequential(*layers)\n","\n","\n","class Bottleneck(nn.Module):\n","  #Densenet bottleneck block\n","    def __init__(self, in_planes, growth_rate):\n","        super(Bottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n","        #Replaced 3x3 convolution with Res2Net block with scale=4\n","        \n","        #Use this for standard Densenet\n","        #self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n","        \n","        #Use this for Dense2net\n","        self.conv2 = Res2Net_block(growth_rate, scale=4, stride=1, groups=1)\n","\n","    def forward(self, x):\n","        out = self.conv1(F.relu(self.bn1(x)))\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = torch.cat([out,x], 1)\n","        return out\n","\n","\n","class Transition(nn.Module):\n","  #Densenet transition layer\n","    def __init__(self, in_planes, out_planes):\n","        super(Transition, self).__init__()\n","        self.bn = nn.BatchNorm2d(in_planes)\n","        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n","\n","    def forward(self, x):\n","        out = self.conv(F.relu(self.bn(x)))\n","        out = F.avg_pool2d(out, 2)\n","        return out\n","\n","\n","class DenseNet(nn.Module):\n","    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=100):\n","        super(DenseNet, self).__init__()\n","        self.growth_rate = growth_rate\n","\n","        num_planes = 2*growth_rate\n","        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n","\n","        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n","        num_planes += nblocks[0]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans1 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n","        num_planes += nblocks[1]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans2 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n","        num_planes += nblocks[2]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans3 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n","        num_planes += nblocks[3]*growth_rate\n","\n","        self.bn = nn.BatchNorm2d(num_planes)\n","        self.linear = nn.Linear(num_planes, num_classes)\n","\n","    def _make_dense_layers(self, block, in_planes, nblock):\n","        layers = []\n","        for i in range(nblock):\n","            layers.append(block(in_planes, self.growth_rate))\n","            in_planes += self.growth_rate\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.trans1(self.dense1(out))\n","        out = self.trans2(self.dense2(out))\n","        out = self.trans3(self.dense3(out))\n","        out = self.dense4(out)\n","        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","def DenseNet121():\n","    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n","\n","def DenseNet169():\n","    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n","\n","def DenseNet201():\n","    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n","\n","def DenseNet161():\n","    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n","\n","def densenet_cifar():\n","    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n","\n","def test():\n","    net = densenet_cifar()\n","    x = torch.randn(1,3,32,32)\n","    y = net(x)\n","    print(y)\n","\n","    \n","print('Defined DenseNet')\n","\n","#For data augmentation\n","data_transforms = {\n","    'Train': transforms.Compose([\n","        #transforms.RandomResizedCrop(224),\n","        #transforms.RandomHorizontalFlip(),\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'Test': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","\n","\n","#Helper functions for training (not currently used)\n","def progress_bar(current, total, msg=None):\n","    global last_time, begin_time\n","    if current == 0:\n","        begin_time = time.time()  # Reset for new bar.\n","\n","    cur_len = int(65*current/total)\n","    rest_len = int(65 - cur_len) - 1\n","\n","    sys.stdout.write(' [')\n","    for i in range(cur_len):\n","        sys.stdout.write('=')\n","    sys.stdout.write('>')\n","    for i in range(rest_len):\n","        sys.stdout.write('.')\n","    sys.stdout.write(']')\n","\n","    cur_time = time.time()\n","    step_time = cur_time - last_time\n","    last_time = cur_time\n","    tot_time = cur_time - begin_time\n","\n","    L = []\n","    L.append('  Step: %s' % format_time(step_time))\n","    L.append(' | Tot: %s' % format_time(tot_time))\n","    if msg:\n","        L.append(' | ' + msg)\n","\n","    msg = ''.join(L)\n","    sys.stdout.write(msg)\n","    for i in range(term_width-int(65)-len(msg)-3):\n","        sys.stdout.write(' ')\n","\n","    # Go back to the center of the bar.\n","    for i in range(term_width-int(65/2)+2):\n","        sys.stdout.write('\\b')\n","    sys.stdout.write(' %d/%d ' % (current+1, total))\n","\n","    if current < total-1:\n","        sys.stdout.write('\\r')\n","    else:\n","        sys.stdout.write('\\n')\n","    sys.stdout.flush()\n","    \n","def format_time(seconds):\n","    days = int(seconds / 3600/24)\n","    seconds = seconds - days*3600*24\n","    hours = int(seconds / 3600)\n","    seconds = seconds - hours*3600\n","    minutes = int(seconds / 60)\n","    seconds = seconds - minutes*60\n","    secondsf = int(seconds)\n","    seconds = seconds - secondsf\n","    millis = int(seconds*1000)\n","\n","    f = ''\n","    i = 1\n","    if days > 0:\n","        f += str(days) + 'D'\n","        i += 1\n","    if hours > 0 and i <= 2:\n","        f += str(hours) + 'h'\n","        i += 1\n","    if minutes > 0 and i <= 2:\n","        f += str(minutes) + 'm'\n","        i += 1\n","    if secondsf > 0 and i <= 2:\n","        f += str(secondsf) + 's'\n","        i += 1\n","    if millis > 0 and i <= 2:\n","        f += str(millis) + 'ms'\n","        i += 1\n","    if f == '':\n","        f = '0ms'\n","    return f\n","print('Defined helper functions')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Defined DenseNet\n","Defined helper functions\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VR08TVXLXIut","colab_type":"text"},"source":["## Training on CIFAR10\n","\n","**To Do:**\n","*   Tune hyperparameters\n","*   Improve training and testing functions (print less often, plot training info, measure runtime and memory for comparison)\n","*   Save best model\n","\n","* Data augmentation (compare with/without?)\n","\n","\n","  "]},{"cell_type":"code","metadata":{"id":"U_1SxNejmlxn","colab_type":"code","outputId":"3031e572-26d6-45b2-a6e2-1aaa9684acfe","executionInfo":{"status":"error","timestamp":1556842077542,"user_tz":240,"elapsed":14619,"user":{"displayName":"Tanner Songkakul","photoUrl":"","userId":"16302517323990624028"}},"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["'''Train CIFAR10 with PyTorch.'''\n","from __future__ import print_function\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import time\n","import sys\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","best_acc = 0  # best test accuracy\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","augment = 1\n","\n","# Data\n","print('==> Preparing data..')\n","if augment:\n","  transform_train = transforms.Compose([\n","      transforms.ToTensor(),\n","      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","  ])\n","else:\n","  transform_train = transforms.Compose([\n","      transforms.RandomCrop(32, padding=4),\n","      transforms.RandomHorizontalFlip(),\n","      transforms.ToTensor(),\n","      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","  ])\n","  \n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","# Model\n","print('==> Building model..')\n","# net = VGG('VGG19')\n","# net = ResNet18()\n","# net = PreActResNet18()\n","# net = GoogLeNet()\n","net = DenseNet121()\n","# net = ResNeXt29_2x64d()\n","# net = MobileNet()\n","# net = MobileNetV2()\n","# net = DPN92()\n","# net = ShuffleNetG2()\n","# net = SENet18()\n","#net = ShuffleNetV2(1)\n","net = net.to(device)\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","''\n","\n","learning_rate = 0.1\n","\n","criterion = nn.CrossEntropyLoss()\n","# NATE: I am surprised they are using SGD, I think we should try with some other optims ADAM in particular\n","optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","        if(batch_idx%25==0) or batch_idx==len(trainloader)-1: \n","          print(batch_idx+1, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","        #progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","        #    % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","            if(batch_idx%25==0) or batch_idx==len(testloader)-1: \n","                print(batch_idx+1, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","            #progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir('checkpoint'):\n","            os.mkdir('checkpoint')\n","        torch.save(state, './checkpoint/ckpt.t7')\n","        best_acc = acc\n","\n","\n","#Variables for progress bar (not currently used)        \n","TOTAL_BAR_LENGTH = 65.\n","last_time = time.time()\n","begin_time = last_time\n","term_width= 1000\n","\n","#train and test\n","for epoch in range(start_epoch, start_epoch+200):\n","    train(epoch)\n","    test(epoch)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n","==> Building model..\n","\n","Epoch: 0\n","1 391 Loss: 4.624 | Acc: 0.781% (1/128)\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-df7b53fe3c83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m#train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-df7b53fe3c83>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"VaVGx1R61oD_","colab_type":"text"},"source":["# Dense2Net-final\n","New version incorporating plotting"]},{"cell_type":"code","metadata":{"id":"cqLiWVHgr6cH","colab_type":"code","outputId":"c9d9ea37-5a76-44ac-d6e8-07faf4e27578","executionInfo":{"status":"error","timestamp":1556900272385,"user_tz":240,"elapsed":3853,"user":{"displayName":"Tanner Songkakul","photoUrl":"","userId":"16302517323990624028"}},"colab":{"base_uri":"https://localhost:8080/","height":540}},"source":["#The below cell is only necessary when running in a Colaboratory notebook\n","\n","\n","# Load the Drive helper and mount \n","from google.colab import drive\n","# # This will prompt for authorization.\n","drive.mount('/content/drive/')\n","# #Navigate to the directory containing this notebook\n","%cd \"/content/drive/My Drive/StarliperSongkakul-Project3/Dense2Net\"\n","\n","\n","!ls\n","\n","import torch"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/My Drive/StarliperSongkakul-Project3/Dense2Net\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n","0502-k14_noaugment_results.pkl\tdense2net_01.py  LICENSE   README.md\n","acc.png\t\t\t\tdense2net_02.py  loss.png  results\n","checkpoint\t\t\tdense2net.ipynb  main.py   utils.py\n","data\t\t\t\tdensenet.ipynb\t models\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-5e2a2a68687a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m __all__ += [name for name in dir(_C)\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             not name.endswith('Base')]\n","\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"]}]},{"cell_type":"code","metadata":{"id":"R2UlHX2_1lto","colab_type":"code","outputId":"3b798f1f-35ff-482b-d51b-1cacc77f576b","executionInfo":{"status":"error","timestamp":1556900243663,"user_tz":240,"elapsed":1820,"user":{"displayName":"Tanner Songkakul","photoUrl":"","userId":"16302517323990624028"}},"colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["from __future__ import print_function\n","\n","# -*- coding: utf-8 -*-\n","\"\"\"dense2net.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1Vv_h6w-_hYBeLLbinWcdgynb0Ro_Ad_D\n","\n","# Dense2Net\n","Implementation incorporating the Res2Net architecture into dense net.\n","\n","DenseNet Paper: https://arxiv.org/abs/1608.06993\n","\n","Res2Net Paper: https://arxiv.org/abs/1904.01169\n","\n","DenseNet Cifar10 code from\n","https://github.com/kuangliu/pytorch-cifar\n","and https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n","\n","Res2Net code from https://github.com/lxtGH/OctaveConv_pytorch/blob/master/nn/res2net.py\n","\n","\n","\n","## Implementation of the Densenet and Res2net blocks\n","**To Do**\n","\n","\n","*   Comment and better understand structure\n","\"\"\"\n","\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import torchvision\n","import torchvision.transforms as transforms\n","import os\n","import time\n","import sys\n","import numpy as np\n","import pickle as pkl\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","\n","def conv3x3(in_planes, out_planes, stride=1, groups=1):  \n","    #returns a 3x3 2d convolution, used in Res2Net sub-convolutions\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, groups=groups, bias=False)\n","\n","class Res2Net_block(nn.Module):\n","    #Res2net bottleneck block\n","    def __init__(self, planes, scale=1, stride=1, groups=1, norm_layer=None):\n","        super(Res2Net_block, self).__init__()\n","        \n","        self.relu = nn.ReLU(inplace=True)\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","            \n","        self.scale = scale\n","        ch_per_sub = planes // self.scale\n","        ch_res = planes % self.scale    \n","        self.chunks  = [ch_per_sub * i + ch_res for i in range(1, scale + 1)]\n","        self.conv_blocks = self.get_sub_convs(ch_per_sub, norm_layer, stride, groups)\n","        \n","    def forward(self, x):\n","        sub_convs = []\n","        sub_convs.append(x[:, :self.chunks[0]])\n","        sub_convs.append(self.conv_blocks[0](x[:, self.chunks[0]: self.chunks[1]]))\n","        for s in range(2, self.scale):\n","            sub_x = x[:, self.chunks[s-1]: self.chunks[s]]\n","            sub_x += sub_convs[-1]\n","            sub_convs.append(self.conv_blocks[s-1](sub_x))\n","\n","        return torch.cat(sub_convs, dim=1)\n","    \n","    def get_sub_convs(self, ch_per_sub, norm_layer, stride, groups):\n","        layers = []\n","        for _ in range(1, self.scale):\n","            layers.append(nn.Sequential(\n","                conv3x3(ch_per_sub, ch_per_sub, stride, groups),\n","                norm_layer(ch_per_sub), self.relu))\n","        \n","        return nn.Sequential(*layers)\n","\n","\n","class Bottleneck(nn.Module):\n","  #Densenet bottleneck block\n","    def __init__(self, in_planes, growth_rate):\n","        super(Bottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n","        #Replaced 3x3 convolution with Res2Net block with scale=4\n","        \n","        #Use this for standard Densenet\n","        #self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n","        \n","        #Use this for Dense2Net\n","        self.conv2 = Res2Net_block(4*growth_rate, scale=4, stride=1, groups=1)\n","        self.conv3 =nn.Conv2d(4*growth_rate, growth_rate, kernel_size=1, bias=False)\n","        \n","    def forward(self, x):\n","        print('bottleneck block 1')\n","        out = self.conv1(F.relu(self.bn1(x)))\n","        print('bottleneck block 2')\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        #Use this for Dense2Net\n","        out = self.conv3(out)\n","        out = torch.cat([out,x], 1)\n","        return out\n","\n","\n","class Transition(nn.Module):\n","  #Densenet transition layer\n","    def __init__(self, in_planes, out_planes):\n","        super(Transition, self).__init__()\n","        self.bn = nn.BatchNorm2d(in_planes)\n","        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n","\n","    def forward(self, x):\n","        out = self.conv(F.relu(self.bn(x)))\n","        out = F.avg_pool2d(out, 2)\n","        return out\n","\n","\n","class DenseNet(nn.Module):\n","    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=100):\n","        super(DenseNet, self).__init__()\n","        self.growth_rate = growth_rate\n","\n","        num_planes = 2*growth_rate\n","        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n","        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n","        num_planes += nblocks[0]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans1 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n","        num_planes += nblocks[1]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans2 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n","        num_planes += nblocks[2]*growth_rate\n","        out_planes = int(math.floor(num_planes*reduction))\n","        self.trans3 = Transition(num_planes, out_planes)\n","        num_planes = out_planes\n","\n","        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n","        num_planes += nblocks[3]*growth_rate\n","\n","        self.bn = nn.BatchNorm2d(num_planes)\n","        self.linear = nn.Linear(num_planes, num_classes)\n","\n","    def _make_dense_layers(self, block, in_planes, nblock):\n","        layers = []\n","        for i in range(nblock):\n","            layers.append(block(in_planes, self.growth_rate))\n","            in_planes += self.growth_rate\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.trans1(self.dense1(out))\n","        out = self.trans2(self.dense2(out))\n","        out = self.trans3(self.dense3(out))\n","        out = self.dense4(out)\n","        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","def DenseNet121():\n","    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n","\n","def DenseNet169():\n","    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n","\n","def DenseNet201():\n","    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n","\n","def DenseNet161():\n","    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=32)\n","\n","def densenet_cifar():\n","    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n","\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    overfit_limit = 99.98\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    avg_loss = 0\n","    acc = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","        acc = 100. * correct / total\n","        avg_loss = train_loss/(batch_idx+1)\n","        if(batch_idx%25==0) or batch_idx==len(trainloader)-1:\n","          print(batch_idx+1, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (avg_loss, acc, correct, total))\n","    \n","    if acc>overfit_limit:\n","        overfit=1\n","        \n","    return avg_loss, acc, overfit\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    avg_loss = 0\n","    acc = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","            acc = 100. * correct / total\n","            avg_loss = test_loss / (batch_idx + 1)\n","            if(batch_idx%25==0) or batch_idx==len(testloader)-1:\n","                print(batch_idx+1, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir('checkpoint'):\n","            os.mkdir('checkpoint')\n","        torch.save(state, './checkpoint/ckpt.t7')\n","        best_acc = acc\n","    return avg_loss, acc\n","\n","\n","def plot_curves(train, test, mode):\n","    if mode == 1:\n","        title = \"Accuracy Curves\"\n","        label = \"Accuracy\"\n","        filename = \"acc.png\"\n","    else:\n","        title = \"Loss Curves\"\n","        label = \"Loss\"\n","        filename = \"loss.png\"\n","    num_epochs_plot = range(0, len(train))  # x axis range\n","    plt.figure()\n","    plt.plot(num_epochs_plot, train, \"b\", label=\"Training\")\n","    plt.plot(num_epochs_plot, test, \"r\", label=\"Validation\")\n","    plt.title(title)\n","    plt.xlabel(\"Number of Epochs\")\n","    plt.ylabel(label)\n","    plt.legend()\n","    plt.savefig(filename)\n","    plt.close()\n","\n","\n","\"\"\"## Training on CIFAR100\n","\n","**To Do:**\n","*   Tune hyperparameters\n","*   Improve training and testing functions (print less often, plot training info, measure runtime and memory for comparison)\n","*   Save best model\n","\n","* Data augmentation (compare with/without?)\n","\"\"\"\n","\n","if __name__ == '__main__':\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    best_acc = 0  # best test accuracy\n","    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","    end_epoch = 100 # number of epochs to run\n","    augment = 0 # set to 1 to augment data\n","    resume = 0 # resume from checkpoint \n","\n","    # Data\n","    print('==> Preparing data..')\n","    if augment==0: #TANNER: this was backwards before, switched in v2\n","      transform_train = transforms.Compose([\n","          transforms.ToTensor(),\n","          #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), #CIFAR10\n","          transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]), #CIFAR100\n","\n","      ])\n","    else:\n","      transform_train = transforms.Compose([\n","          transforms.RandomCrop(32, padding=4),\n","          transforms.RandomHorizontalFlip(),\n","          transforms.ToTensor(),\n","          #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), #CIFAR10\n","          transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]), #CIFAR100\n","      ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), #CIFAR10\n","        transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]), #CIFAR100\n","    ])\n","\n","    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","    # classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    # Model\n","    print('==> Building model..')\n","    net = DenseNet121()\n","    net = net.to(device)\n","    if device == 'cuda':\n","        net = nn.DataParallel(net)\n","        cudnn.benchmark = True\n","    \n","    learning_rate = 0.1\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001) #(tanner): changed weight decay to 0.0001 to match Res2net paper\n","\n","\n","    train_loss = np.zeros((end_epoch, 1))\n","    train_acc = np.zeros((end_epoch, 1))\n","    test_acc = np.zeros((end_epoch, 1))\n","    test_loss = np.zeros((end_epoch, 1))\n","\n","    if resume == 1:\n","\t    # Load checkpoint.\n","\t    print('==> Resuming from checkpoint..')\n","\t    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n","\t    checkpoint = torch.load('./checkpoint/ckpt.t7')\n","\t    net.load_state_dict(checkpoint['net'])\n","\t    best_acc = checkpoint['acc']\n","\t    start_epoch = checkpoint['epoch']\n","\t    with open('results.pkl', 'rb') as file:\n","      \t\ttrain_loss, train_acc, test_loss, test_acc = pkl.load(file)\n","\n","    track_overfit = 0\n","    #train and test\n","    start_time = time.time()\n","    for epoch in range(start_epoch, end_epoch):\n","      if (epoch == 30) or (epoch == 60):#((epoch+1)%30)==0:\n","          for g in optimizer.param_groups:\n","            learning_rate /= 10\n","            g['lr'] = learning_rate #reduce learning rate by a factor of 10 every 30 epochs\n","            print('learning rate reduced to '+str(learning_rate))\n","\n","      train_loss[epoch], train_acc[epoch],overfit_check = train(epoch)\n","      test_loss[epoch], test_acc[epoch] = test(epoch)\n","      results = [train_loss, train_acc, test_loss, test_acc]\n","      with open('results.pkl', 'wb') as file:\n","          pkl.dump(results, file)\n","      track_overfit+= overfit_check\n","      if track_overfit>2:\n","          print('overfit, terminating run')\n","          break\n","    #plot training and testing curves\n","    plot_curves(train_loss, test_loss, 0)\n","    plot_curves(train_acc, test_acc, 1)\n","    end_time = time.time()\n","    print('Total Time: ', (end_time - start_time))\n"],"execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-eaafd5c8e659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m __all__ += [name for name in dir(_C)\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             not name.endswith('Base')]\n","\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"]}]}]}